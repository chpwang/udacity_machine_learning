### 说明

- `Agent.py`：在此处开发强化学习智能体。这是你唯一需要修改的文件。
- `Monitor.py`：interact 函数测试智能体通过与环境互动达到的学习效果。
- `main.py`：在终端里运行此文件，以检查智能体的性能。


当你运行 `main.py` 时，你在 `Agent.py` 中指定的智能体会与环境互动 20,000 个阶段。互动详情在 `Monitor.py` 中指定，它会返回两个变量：`avg_rewards` 和 `best_avg_reward`：

- `avg_rewards` 是一个双队列，其中 `avg_rewards[i]` 是智能体从阶段 `i+1` 到阶段 `i+100`（含）收集的平均（未折扣）回报。例如，`avg_rewards[0]` 是智能体在前 100 个阶段中收集的平均回报
- `best_avg_reward` 是 `avg_rewards` 中的最大项。这是你在确定智能体在该任务中的效果时应该使用的最终得分

---
### 任务目标

你的任务是修改 `Agents.py` 文件以改进智能体的性能。

- 使用 `__init__()` 方法定义任何所需的实例变量。目前，我们定义了智能体可以采取的动作数量 (`nA`) ，并将动作值 (`Q`) 初始化为空的数组字典。你可以随意添加更多实例变量；例如，如果智能体使用 Epsilon 贪婪策略选择动作，你可能会发现有必要定义 **ε** 的值。
- `select_action()` 方法将环境状态作为输入，并返回智能体选择的动作。我们提供的默认代码会随机地选择动作。
- `step()` 方法将（`state`、`action`、`reward`、`next_state`）元组以及 `done` 变量作为输入，如果 **ε** 已结束，该变量将为 `True`。默认代码（你肯定需要更改！）会使上个状态动作对的值加一。你应该更改此方法，以使用抽取的经验元组更新智能体对问题的了解。

修改该函数后，你只需运行 python main.py 以测试新智能体。

虽然你可以实现你所选的任何算法，但是注意，你可以通过使用我们在课程中介绍的一些方法达到满意的性能。

---
### 评估性能

OpenAI Gym 将[“解决”](https://gym.openai.com/envs/Taxi-v1/)该任务定义为在连续尝试 100 次以上之后平均回报为 9.7。

虽然我们不会给此迷你项目打分，但是，建议你在连续尝试 100 次以上之后至少达到 9.1（`best_avg_reward` > 9.1）。

最后一步，为了与更广泛的 RL 社区分享你的观点，可以写一个总结并提交到 [OpenAI Gym Leaderboard](https://github.com/openai/gym/wiki/Leaderboard)！